{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Deep learning with Keras</center></h1>\n",
    "\n",
    "<center>Owen Jones | Bath ML | 3rd June 2018</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start at the very beginning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. -- **[keras.io](https://keras.io)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, Keras makes it super easy to build neural networks. And that is exactly what we're going to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jones\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best dataset in the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the hang of the Keras syntax, we're going to start off with a really simple network on a really simple dataset. You might have seen this one before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "import numpy as np\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 0. ],\n",
       "       [4.9, 3. , 1.4, 0.2, 0. ],\n",
       "       [4.7, 3.2, 1.3, 0.2, 0. ],\n",
       "       [4.6, 3.1, 1.5, 0.2, 0. ],\n",
       "       [5. , 3.6, 1.4, 0.2, 0. ],\n",
       "       [5.4, 3.9, 1.7, 0.4, 0. ],\n",
       "       [4.6, 3.4, 1.4, 0.3, 0. ],\n",
       "       [5. , 3.4, 1.5, 0.2, 0. ],\n",
       "       [4.4, 2.9, 1.4, 0.2, 0. ],\n",
       "       [4.9, 3.1, 1.5, 0.1, 0. ],\n",
       "       [5.4, 3.7, 1.5, 0.2, 0. ],\n",
       "       [4.8, 3.4, 1.6, 0.2, 0. ],\n",
       "       [4.8, 3. , 1.4, 0.1, 0. ],\n",
       "       [4.3, 3. , 1.1, 0.1, 0. ],\n",
       "       [5.8, 4. , 1.2, 0.2, 0. ],\n",
       "       [5.7, 4.4, 1.5, 0.4, 0. ],\n",
       "       [5.4, 3.9, 1.3, 0.4, 0. ],\n",
       "       [5.1, 3.5, 1.4, 0.3, 0. ],\n",
       "       [5.7, 3.8, 1.7, 0.3, 0. ],\n",
       "       [5.1, 3.8, 1.5, 0.3, 0. ],\n",
       "       [5.4, 3.4, 1.7, 0.2, 0. ],\n",
       "       [5.1, 3.7, 1.5, 0.4, 0. ],\n",
       "       [4.6, 3.6, 1. , 0.2, 0. ],\n",
       "       [5.1, 3.3, 1.7, 0.5, 0. ],\n",
       "       [4.8, 3.4, 1.9, 0.2, 0. ],\n",
       "       [5. , 3. , 1.6, 0.2, 0. ],\n",
       "       [5. , 3.4, 1.6, 0.4, 0. ],\n",
       "       [5.2, 3.5, 1.5, 0.2, 0. ],\n",
       "       [5.2, 3.4, 1.4, 0.2, 0. ],\n",
       "       [4.7, 3.2, 1.6, 0.2, 0. ],\n",
       "       [4.8, 3.1, 1.6, 0.2, 0. ],\n",
       "       [5.4, 3.4, 1.5, 0.4, 0. ],\n",
       "       [5.2, 4.1, 1.5, 0.1, 0. ],\n",
       "       [5.5, 4.2, 1.4, 0.2, 0. ],\n",
       "       [4.9, 3.1, 1.5, 0.1, 0. ],\n",
       "       [5. , 3.2, 1.2, 0.2, 0. ],\n",
       "       [5.5, 3.5, 1.3, 0.2, 0. ],\n",
       "       [4.9, 3.1, 1.5, 0.1, 0. ],\n",
       "       [4.4, 3. , 1.3, 0.2, 0. ],\n",
       "       [5.1, 3.4, 1.5, 0.2, 0. ],\n",
       "       [5. , 3.5, 1.3, 0.3, 0. ],\n",
       "       [4.5, 2.3, 1.3, 0.3, 0. ],\n",
       "       [4.4, 3.2, 1.3, 0.2, 0. ],\n",
       "       [5. , 3.5, 1.6, 0.6, 0. ],\n",
       "       [5.1, 3.8, 1.9, 0.4, 0. ],\n",
       "       [4.8, 3. , 1.4, 0.3, 0. ],\n",
       "       [5.1, 3.8, 1.6, 0.2, 0. ],\n",
       "       [4.6, 3.2, 1.4, 0.2, 0. ],\n",
       "       [5.3, 3.7, 1.5, 0.2, 0. ],\n",
       "       [5. , 3.3, 1.4, 0.2, 0. ],\n",
       "       [7. , 3.2, 4.7, 1.4, 1. ],\n",
       "       [6.4, 3.2, 4.5, 1.5, 1. ],\n",
       "       [6.9, 3.1, 4.9, 1.5, 1. ],\n",
       "       [5.5, 2.3, 4. , 1.3, 1. ],\n",
       "       [6.5, 2.8, 4.6, 1.5, 1. ],\n",
       "       [5.7, 2.8, 4.5, 1.3, 1. ],\n",
       "       [6.3, 3.3, 4.7, 1.6, 1. ],\n",
       "       [4.9, 2.4, 3.3, 1. , 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3, 1. ],\n",
       "       [5.2, 2.7, 3.9, 1.4, 1. ],\n",
       "       [5. , 2. , 3.5, 1. , 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5, 1. ],\n",
       "       [6. , 2.2, 4. , 1. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4, 1. ],\n",
       "       [5.6, 2.9, 3.6, 1.3, 1. ],\n",
       "       [6.7, 3.1, 4.4, 1.4, 1. ],\n",
       "       [5.6, 3. , 4.5, 1.5, 1. ],\n",
       "       [5.8, 2.7, 4.1, 1. , 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5, 1. ],\n",
       "       [5.6, 2.5, 3.9, 1.1, 1. ],\n",
       "       [5.9, 3.2, 4.8, 1.8, 1. ],\n",
       "       [6.1, 2.8, 4. , 1.3, 1. ],\n",
       "       [6.3, 2.5, 4.9, 1.5, 1. ],\n",
       "       [6.1, 2.8, 4.7, 1.2, 1. ],\n",
       "       [6.4, 2.9, 4.3, 1.3, 1. ],\n",
       "       [6.6, 3. , 4.4, 1.4, 1. ],\n",
       "       [6.8, 2.8, 4.8, 1.4, 1. ],\n",
       "       [6.7, 3. , 5. , 1.7, 1. ],\n",
       "       [6. , 2.9, 4.5, 1.5, 1. ],\n",
       "       [5.7, 2.6, 3.5, 1. , 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1, 1. ],\n",
       "       [5.5, 2.4, 3.7, 1. , 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2, 1. ],\n",
       "       [6. , 2.7, 5.1, 1.6, 1. ],\n",
       "       [5.4, 3. , 4.5, 1.5, 1. ],\n",
       "       [6. , 3.4, 4.5, 1.6, 1. ],\n",
       "       [6.7, 3.1, 4.7, 1.5, 1. ],\n",
       "       [6.3, 2.3, 4.4, 1.3, 1. ],\n",
       "       [5.6, 3. , 4.1, 1.3, 1. ],\n",
       "       [5.5, 2.5, 4. , 1.3, 1. ],\n",
       "       [5.5, 2.6, 4.4, 1.2, 1. ],\n",
       "       [6.1, 3. , 4.6, 1.4, 1. ],\n",
       "       [5.8, 2.6, 4. , 1.2, 1. ],\n",
       "       [5. , 2.3, 3.3, 1. , 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3, 1. ],\n",
       "       [5.7, 3. , 4.2, 1.2, 1. ],\n",
       "       [5.7, 2.9, 4.2, 1.3, 1. ],\n",
       "       [6.2, 2.9, 4.3, 1.3, 1. ],\n",
       "       [5.1, 2.5, 3. , 1.1, 1. ],\n",
       "       [5.7, 2.8, 4.1, 1.3, 1. ],\n",
       "       [6.3, 3.3, 6. , 2.5, 2. ],\n",
       "       [5.8, 2.7, 5.1, 1.9, 2. ],\n",
       "       [7.1, 3. , 5.9, 2.1, 2. ],\n",
       "       [6.3, 2.9, 5.6, 1.8, 2. ],\n",
       "       [6.5, 3. , 5.8, 2.2, 2. ],\n",
       "       [7.6, 3. , 6.6, 2.1, 2. ],\n",
       "       [4.9, 2.5, 4.5, 1.7, 2. ],\n",
       "       [7.3, 2.9, 6.3, 1.8, 2. ],\n",
       "       [6.7, 2.5, 5.8, 1.8, 2. ],\n",
       "       [7.2, 3.6, 6.1, 2.5, 2. ],\n",
       "       [6.5, 3.2, 5.1, 2. , 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9, 2. ],\n",
       "       [6.8, 3. , 5.5, 2.1, 2. ],\n",
       "       [5.7, 2.5, 5. , 2. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4, 2. ],\n",
       "       [6.4, 3.2, 5.3, 2.3, 2. ],\n",
       "       [6.5, 3. , 5.5, 1.8, 2. ],\n",
       "       [7.7, 3.8, 6.7, 2.2, 2. ],\n",
       "       [7.7, 2.6, 6.9, 2.3, 2. ],\n",
       "       [6. , 2.2, 5. , 1.5, 2. ],\n",
       "       [6.9, 3.2, 5.7, 2.3, 2. ],\n",
       "       [5.6, 2.8, 4.9, 2. , 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. , 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8, 2. ],\n",
       "       [6.7, 3.3, 5.7, 2.1, 2. ],\n",
       "       [7.2, 3.2, 6. , 1.8, 2. ],\n",
       "       [6.2, 2.8, 4.8, 1.8, 2. ],\n",
       "       [6.1, 3. , 4.9, 1.8, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.1, 2. ],\n",
       "       [7.2, 3. , 5.8, 1.6, 2. ],\n",
       "       [7.4, 2.8, 6.1, 1.9, 2. ],\n",
       "       [7.9, 3.8, 6.4, 2. , 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2, 2. ],\n",
       "       [6.3, 2.8, 5.1, 1.5, 2. ],\n",
       "       [6.1, 2.6, 5.6, 1.4, 2. ],\n",
       "       [7.7, 3. , 6.1, 2.3, 2. ],\n",
       "       [6.3, 3.4, 5.6, 2.4, 2. ],\n",
       "       [6.4, 3.1, 5.5, 1.8, 2. ],\n",
       "       [6. , 3. , 4.8, 1.8, 2. ],\n",
       "       [6.9, 3.1, 5.4, 2.1, 2. ],\n",
       "       [6.7, 3.1, 5.6, 2.4, 2. ],\n",
       "       [6.9, 3.1, 5.1, 2.3, 2. ],\n",
       "       [5.8, 2.7, 5.1, 1.9, 2. ],\n",
       "       [6.8, 3.2, 5.9, 2.3, 2. ],\n",
       "       [6.7, 3.3, 5.7, 2.5, 2. ],\n",
       "       [6.7, 3. , 5.2, 2.3, 2. ],\n",
       "       [6.3, 2.5, 5. , 1.9, 2. ],\n",
       "       [6.5, 3. , 5.2, 2. , 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3, 2. ],\n",
       "       [5.9, 3. , 5.1, 1.8, 2. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = np.load(\"data/iris.npy\")\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four columns are numeric features (plant-related measurements... don't worry too much), and the fifth column is a label corresponding to the species, which is what we're going to use as our target.\n",
    "\n",
    "First we're just going to shuffle the rows, because at the moment they're in order (notice the label in the last column); in a minute we'll be splitting the data and we want a mixture of labels in each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll separate the labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_labels = iris[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... because Keras needs the labels to be \"one-hot encoded\".\n",
    "\n",
    "**One-hot encoded label:** list out all the possible labels, and mark the one which is correct.\n",
    "\n",
    "Here, our label could be 0, 1 or 2. \n",
    "    Is it: 0? 1? 2?\n",
    "    ---------------\n",
    "    0 =>  [1, 0, 0]\n",
    "    1 =>  [0, 1, 0]\n",
    "    2 =>  [0, 0, 1]\n",
    "\n",
    "Keras can do this for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "iris_onehot = to_categorical(iris_labels)\n",
    "iris_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get on with building a neural net!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a \"sequential\" model. The clue's in the name - we start with an empty model, and _sequentially_ add layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of layers to choose from, and we'll see some more exciting ones later on, but for now we'll stick with dense layers.\n",
    "\n",
    "For each dense layer, we specify:\n",
    "* The number of `units`, or how many neurons we want in the layer - the final layer will need to have 3 units, because we're classifying as one of 3 labels\n",
    "* The `activation` function we want to use - in a fully dense network, we tend to use sigmoid activation on the middle layers and softmax on the output layer\n",
    "\n",
    "And Keras takes care of everything else for us.\n",
    "\n",
    "Well, almost. For the very first layer in any model, we need to tell Keras what \"shape\" our input will be. We ignore the first dimension (the number of observations, or \"rows\"), because it can change without consequence; but we have to specify the other dimensions in a tuple. Here, it's a size-1 tuple, specifying that we have 4 features (\"columns\").\n",
    "\n",
    "Oh, and to add layers we use... umm, `add()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(15, activation=\"sigmoid\", input_shape=(4,)))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've created..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 15)                75        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 48        \n",
      "=================================================================\n",
      "Total params: 123\n",
      "Trainable params: 123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "There's one more thing to do before we can train our model though: now that we're happy with it, we have to lock it in by \"compiling\" it.\n",
    "\n",
    "At this point we also need to tell Keras which loss function (always categorical crossentropy for multiclass classification) and optimizer (stochastic gradient descent, or something more fancy) to use.\n",
    "\n",
    "We can also ask for a list of metrics which we would like to see reported during training. These have nothing to do with the training process. The training process tries to minimise the loss function. Usually that results in an increase in accuracy. Once again. The metrics have NOTHING to do with the training process. It's just nice to see them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit our model! We pass in our training data (remember that the label is in the last column, and we don't want to include that here!) and our one-hot encoded labels, as well as:\n",
    "* The number of `epochs` to train for, or the total number of times that all our training data gets passed through the network\n",
    "* A \"batch size\" - the parameters in the network get updated after each `batch_size` observations have been passed through\n",
    "* A proportion of the data which we'll set aside and use to assess our model's generalised performance (because a model can usually make great predictions about the data that's been used to train it, but it might not do so well on data it hasn't seen before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.1065 - acc: 0.4333 - val_loss: 1.1507 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "120/120 [==============================] - 0s 100us/step - loss: 1.0938 - acc: 0.6083 - val_loss: 1.1360 - val_acc: 0.5333\n",
      "Epoch 3/50\n",
      "120/120 [==============================] - 0s 125us/step - loss: 1.0843 - acc: 0.6167 - val_loss: 1.1239 - val_acc: 0.4667\n",
      "Epoch 4/50\n",
      "120/120 [==============================] - 0s 167us/step - loss: 1.0777 - acc: 0.4750 - val_loss: 1.1134 - val_acc: 0.3000\n",
      "Epoch 5/50\n",
      "120/120 [==============================] - 0s 125us/step - loss: 1.0723 - acc: 0.4250 - val_loss: 1.1050 - val_acc: 0.2667\n",
      "Epoch 6/50\n",
      "120/120 [==============================] - 0s 125us/step - loss: 1.0657 - acc: 0.3917 - val_loss: 1.0979 - val_acc: 0.2333\n",
      "Epoch 7/50\n",
      "120/120 [==============================] - 0s 125us/step - loss: 1.0610 - acc: 0.4917 - val_loss: 1.0917 - val_acc: 0.2667\n",
      "Epoch 8/50\n",
      "120/120 [==============================] - 0s 125us/step - loss: 1.0571 - acc: 0.3833 - val_loss: 1.0862 - val_acc: 0.2667\n",
      "Epoch 9/50\n",
      "120/120 [==============================] - 0s 134us/step - loss: 1.0542 - acc: 0.3917 - val_loss: 1.0814 - val_acc: 0.2667\n",
      "Epoch 10/50\n",
      "120/120 [==============================] - 0s 151us/step - loss: 1.0511 - acc: 0.3833 - val_loss: 1.0771 - val_acc: 0.2667\n",
      "Epoch 11/50\n",
      "120/120 [==============================] - 0s 134us/step - loss: 1.0468 - acc: 0.4083 - val_loss: 1.0731 - val_acc: 0.2667\n",
      "Epoch 12/50\n",
      "120/120 [==============================] - 0s 150us/step - loss: 1.0446 - acc: 0.4250 - val_loss: 1.0692 - val_acc: 0.2667\n",
      "Epoch 13/50\n",
      "120/120 [==============================] - 0s 142us/step - loss: 1.0409 - acc: 0.4083 - val_loss: 1.0653 - val_acc: 0.3000\n",
      "Epoch 14/50\n",
      "120/120 [==============================] - 0s 159us/step - loss: 1.0383 - acc: 0.3917 - val_loss: 1.0614 - val_acc: 0.3333\n",
      "Epoch 15/50\n",
      "120/120 [==============================] - 0s 201us/step - loss: 1.0349 - acc: 0.4417 - val_loss: 1.0579 - val_acc: 0.4000\n",
      "Epoch 16/50\n",
      "120/120 [==============================] - 0s 125us/step - loss: 1.0314 - acc: 0.5083 - val_loss: 1.0545 - val_acc: 0.5000\n",
      "Epoch 17/50\n",
      "120/120 [==============================] - 0s 159us/step - loss: 1.0284 - acc: 0.5167 - val_loss: 1.0508 - val_acc: 0.5667\n",
      "Epoch 18/50\n",
      "120/120 [==============================] - 0s 167us/step - loss: 1.0254 - acc: 0.5917 - val_loss: 1.0470 - val_acc: 0.6333\n",
      "Epoch 19/50\n",
      "120/120 [==============================] - 0s 142us/step - loss: 1.0222 - acc: 0.6500 - val_loss: 1.0435 - val_acc: 0.6667\n",
      "Epoch 20/50\n",
      "120/120 [==============================] - 0s 117us/step - loss: 1.0188 - acc: 0.7667 - val_loss: 1.0400 - val_acc: 0.7000\n",
      "Epoch 21/50\n",
      "120/120 [==============================] - 0s 134us/step - loss: 1.0140 - acc: 0.7500 - val_loss: 1.0361 - val_acc: 0.7000\n",
      "Epoch 22/50\n",
      "120/120 [==============================] - 0s 142us/step - loss: 1.0107 - acc: 0.7250 - val_loss: 1.0321 - val_acc: 0.7000\n",
      "Epoch 23/50\n",
      "120/120 [==============================] - 0s 142us/step - loss: 1.0070 - acc: 0.7500 - val_loss: 1.0279 - val_acc: 0.7667\n",
      "Epoch 24/50\n",
      "120/120 [==============================] - 0s 150us/step - loss: 1.0026 - acc: 0.7750 - val_loss: 1.0232 - val_acc: 0.8000\n",
      "Epoch 25/50\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.9971 - acc: 0.7917 - val_loss: 1.0188 - val_acc: 0.7667\n",
      "Epoch 26/50\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.9921 - acc: 0.7917 - val_loss: 1.0139 - val_acc: 0.7333\n",
      "Epoch 27/50\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.9884 - acc: 0.7583 - val_loss: 1.0091 - val_acc: 0.7333\n",
      "Epoch 28/50\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.9819 - acc: 0.7917 - val_loss: 1.0039 - val_acc: 0.7333\n",
      "Epoch 29/50\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.9768 - acc: 0.8083 - val_loss: 0.9990 - val_acc: 0.7333\n",
      "Epoch 30/50\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.9719 - acc: 0.7583 - val_loss: 0.9939 - val_acc: 0.7333\n",
      "Epoch 31/50\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.9669 - acc: 0.7583 - val_loss: 0.9888 - val_acc: 0.7333\n",
      "Epoch 32/50\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.9619 - acc: 0.7667 - val_loss: 0.9836 - val_acc: 0.7000\n",
      "Epoch 33/50\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.9559 - acc: 0.8083 - val_loss: 0.9791 - val_acc: 0.7000\n",
      "Epoch 34/50\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.9513 - acc: 0.7583 - val_loss: 0.9745 - val_acc: 0.7000\n",
      "Epoch 35/50\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.9456 - acc: 0.7833 - val_loss: 0.9700 - val_acc: 0.7000\n",
      "Epoch 36/50\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.9413 - acc: 0.7917 - val_loss: 0.9656 - val_acc: 0.7000\n",
      "Epoch 37/50\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.9363 - acc: 0.7500 - val_loss: 0.9616 - val_acc: 0.7000\n",
      "Epoch 38/50\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.9324 - acc: 0.7583 - val_loss: 0.9579 - val_acc: 0.7333\n",
      "Epoch 39/50\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.9281 - acc: 0.7750 - val_loss: 0.9543 - val_acc: 0.7333\n",
      "Epoch 40/50\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.9233 - acc: 0.7750 - val_loss: 0.9505 - val_acc: 0.7000\n",
      "Epoch 41/50\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.9190 - acc: 0.8000 - val_loss: 0.9473 - val_acc: 0.7000\n",
      "Epoch 42/50\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.9147 - acc: 0.7667 - val_loss: 0.9440 - val_acc: 0.7000\n",
      "Epoch 43/50\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.9103 - acc: 0.7833 - val_loss: 0.9406 - val_acc: 0.7000\n",
      "Epoch 44/50\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.9071 - acc: 0.7917 - val_loss: 0.9376 - val_acc: 0.7000\n",
      "Epoch 45/50\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.9040 - acc: 0.8000 - val_loss: 0.9338 - val_acc: 0.7000\n",
      "Epoch 46/50\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.8995 - acc: 0.7250 - val_loss: 0.9307 - val_acc: 0.7000\n",
      "Epoch 47/50\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.8960 - acc: 0.7750 - val_loss: 0.9277 - val_acc: 0.7000\n",
      "Epoch 48/50\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.8912 - acc: 0.7750 - val_loss: 0.9248 - val_acc: 0.7333\n",
      "Epoch 49/50\n",
      "120/120 [==============================] - 0s 284us/step - loss: 0.8884 - acc: 0.7917 - val_loss: 0.9223 - val_acc: 0.7333\n",
      "Epoch 50/50\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.8853 - acc: 0.7417 - val_loss: 0.9194 - val_acc: 0.7333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b79a626a58>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(iris[:, :4], iris_onehot, epochs=50, batch_size=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the loss keeps dropping, and the accuracy fluctuates but generally speaking increases for both the training and validation sets.\n",
    "\n",
    "So, that's the syntax, but let's be honest - it's a rubbish boring network and a rubbish boring dataset. Let's move on to something more interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Identify person based on gait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walking = np.load(\"data/walking_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "walking_labels = np.load(\"data/walking_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = walking.shape[0]\n",
    "from random import shuffle\n",
    "indices = [x for x in range(m)]\n",
    "shuffle(indices)\n",
    "train_indices = indices[:int(m*0.6)]\n",
    "val_indices = indices[int(m*0.6):int(m*0.8)]\n",
    "test_indices = indices[int(m*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = walking[train_indices, :, :]\n",
    "X_val = walking[val_indices, :, :]\n",
    "X_test = walking[test_indices, :, :]\n",
    "\n",
    "# We have 15 integer labels, but these need to be one-hot encoded\n",
    "# e.g. '4' becomes [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "y_train = to_categorical(walking_labels[train_indices])\n",
    "y_val = to_categorical(walking_labels[val_indices])\n",
    "y_test = to_categorical(walking_labels[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's have a little look...\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_series(series):\n",
    "    plt.plot(series[:, 0], color=\"red\")\n",
    "    plt.plot(series[:, 1], color=\"green\")\n",
    "    plt.plot(series[:, 2], color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_series(X_train[0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we tell between different people's data by eye?\n",
    "\n",
    "Let's plot a few series for some different people - say, 5 series for 3 people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](three.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate the model - we'll use a sequential model so we can add to it\n",
    "model = Sequential()\n",
    "\n",
    "# Start with a convolutional layer:\n",
    "#  * filters: The number of \"features\" we want to learn; number of patterns to try to identify\n",
    "#  * kernel_size: The \"window\" to consider, i.e. we look at a rolling window captuiring [kernel_size] time points at once\n",
    "#  * strides: How many time steps to \"roll forward\" each time we move the window\n",
    "#  * activation: The activation function to use; convolutional layers typically use REctified Linear Unit function\n",
    "#  * input_shape: We're feeding in observations each of shape 260{time points}*3{directional acceleration features}\n",
    "model.add(Conv1D(filters=40, kernel_size=40, strides=2, activation=\"relu\", input_shape=(260, 3)))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "print(model.output_shape)\n",
    "\n",
    "# Another convolutional layer: this one finds \"meta-patterns\" in the patterns the first layer picked up\n",
    "model.add(Conv1D(filters=40, kernel_size=10, activation=\"relu\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "# If the net is too large and computation too slow, we can reduce the number of parameters with max pooling\n",
    "# This layer would reduce the number of parameters by half by combining (\"pooling\") parameters\n",
    "# i.e. parameters get paired up (by position) and the maximum one only is kept\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "print(model.output_shape)\n",
    "\n",
    "# We still have a 3-dimensional set of parameters - we need to make this 2-dimensional, so we \"flatten\"\n",
    "# (Unstack all the leaves and lay them out next to each other)\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "\n",
    "# We need to finish with a couple of dense layers: one to detect relationships between the (flattened)\n",
    "# convolutional neurons, and...\n",
    "model.add(Dense(100, activation=\"sigmoid\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "# ... one to present the output as a one-hot vector.\n",
    "# (We typically use softmax in the very final layer since it provides a \"stronger\" signal than sigmoid)\n",
    "model.add(Dense(15, activation=\"softmax\"))\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to compile the network before we can run it, defining:\n",
    "# * Loss function to use (always categorical cross-entropy for multi-class logistic regression)\n",
    "# * Optimizer to use\n",
    "#   (\"adam\" = \"ADAptive Movement estimation\", but e.g. \"sgd\" = \"Stochastic Gradient Descent\" will work, just slower)\n",
    "# * Metrics to report (NOT used for adjusting parameters - that's what the loss function is for!)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit it!\n",
    "# * X_train and y_train are training data/labels\n",
    "# * epochs: How many times to pass the training data through and update the network's parameters\n",
    "# * batch_size: How many observations to include in each batch the optimizer sees\n",
    "# * Also show us the accuracy for the cross-validation set\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we could try to improve that cross-validation accuracy score, e.g. change network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to visualise the \"features\" of the time series which the convolutional layers of the net have learned to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the weights from a given layer\n",
    "def plot_filter(model, layer, k):\n",
    "    x = model.layers[layer].get_weights()[0][:, :, k]\n",
    "    plot_series(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filter(model, 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also see if there are any patterns in the autocorrelation plots which might suggest strong periodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_filter_corr(model, layer, k):\n",
    "    weights = model.layers[layer].get_weights()[0][:, :, k]\n",
    "    corrs = np.apply_along_axis(lambda y: np.correlate(y, y, mode=\"full\"), 0, weights)\n",
    "    plot_series(corrs[corrs.shape[0]//2:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filter_corr(model, 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot each filter with its autocorrelation plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, nrows=2, figsize=(50, 20))\n",
    "\n",
    "for k in range(5):\n",
    "    plt.subplot(2, 5, 1+k)\n",
    "    plot_filter(model, 0, k)\n",
    "    plt.subplot(2, 5, 6+k)\n",
    "    plot_filter_corr(model, 0, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"corrs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
