---
title: "Deep learning with Keras"
author: "Owen Jones | BathML | 3rd June 2018"
output: html_notebook
---


Let's start at the very beginning...

```{r}
set.seed(2018)
```

OK yes, that is not the most interesting beginning. But it means that the results in this notebook are now reproducible (yay!). Good. Moving on.



## What is Keras?

> Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. -- **[keras.io](https://keras.io)**

In other words, Keras makes it super easy to build neural networks. And that is exactly what we're going to do.

```{r}
library(keras)
```


---


## Cue the mandolin

To get the hang of the Keras syntax, we're going to start off with a really simple network on a really simple dataset. You might have come across this one before...

```{r}
iris
```

The first four columns are numeric features (plant-related measurements... don't worry too much), and the fifth column is a label corresponding to the species, which is what we're going to use as our target.

First we're just going to shuffle the rows, because at the moment they're in order (notice the label in the last column); in a minute we'll be splitting the data and we want a mixture of labels in each part.

```{r}
iris <- iris[sample(nrow(iris)), ]
```

Now we'll separate the labels, make them numeric, and subtract 1...

```{r}
iris_labels <- as.numeric(iris$Species) - 1
```

... because Keras needs the labels to be "one-hot encoded".

**One-hot encoded label:** list out all the possible labels, and mark the one which is correct.

Here, our label could be "setosa", "versicolor" or "virginica".  

    Is it: s?ve?vi?
    ---------------
    s  => [1, 0, 0]
    ve => [0, 1, 0]
    vi => [0, 0, 1]

Keras can do this for us...

```{r}
iris_onehot <- to_categorical(iris$Species)
iris_onehot
```

Wait - what's with this "and subtract 1"?!

Yes, we're using R, but behind the scenes Keras is actually sneakily running in Python. The `keras` package, which is just an interface through to Python, usually does a pretty good job of hiding all the Python things; but occasionally the mask slips a little! In R we start counting from 1, so when we convert our `Species` labels to numeric format, "setosa" becomes 1, "versicolor" becomes 2 and "virginica" becomes 3. But `to_categorical()` starts counting from 0, because... Python. So we have to play nice and reduce all our labels by 1.

OK great - now we can get on with building a neural net!



```{r}

model <- keras_model_sequential()

model %>%
  layer_dense(10, activation = "sigmoid", input_shape = 4) %>%
  layer_dense(3, activation = "softmax")
```

```{r}
summary(model)
```


```{r}
model %>%
  compile(loss = "categorical_crossentropy", optimizer = "sgd", metrics = c("accuracy"))

```

```{r}
model %>%
  fit(as.matrix(scale(iris[, -5])), iris_onehot, epochs = 20, batch_size = 20, validation_split = 0.2)
```

---

## Identify a person based on gait

```{r}
walking <- readRDS("data/walking_data.rds")
walking_labels <- readRDS("data/walking_labels.rds")
```

### Preparing the data

```{r}
m <- nrow(walking)

indices <- sample(1:m, m)

train_indices <- indices[1:floor(m*0.6)]
val_indices <- indices[ceiling(m*0.6):floor(m*0.8)]
test_indices <- indices[ceiling(m*0.8):m]
```

```{r}
X_train <- walking[train_indices, , ]
X_val <- walking[val_indices, , ]
X_test <- walking[test_indices, , ]

# We have 15 integer labels, but these need to be one-hot encoded
# e.g. '4' becomes [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
y_train <- to_categorical(walking_labels[train_indices])
y_val <- to_categorical(walking_labels[val_indices])
y_test <- to_categorical(walking_labels[test_indices])
```


```{r}
plot_series <- function(series) {
  plot(series[, 1], type = "l", col = "red")
  lines(series[, 2], col = "darkgreen")
  lines(series[, 3], col = "blue")
}

plot_series(X_train[1, , ])
```

Can we tell between different people's data by eye?


### The neural network

```{r}
# Initiate the model - we'll use a sequential model so we can add to it
model <- keras_model_sequential()

# Start with a convolutional layer:
#  * filters: The number of "features" we want to learn; number of patterns to try to identify
#  * kernel_size: The "window" to consider, i.e. we look at a rolling window captuiring [kernel_size] time points at once
#  * strides: How many time steps to "roll forward" each time we move the window
#  * activation: The activation function to use; convolutional layers typically use REctified Linear Unit function
#  * input_shape: We're feeding in observations each of shape 260{time points}*3{directional acceleration features}
model %>%
    layer_conv_1d(filters = 40, kernel_size = 30, strides = 2, activation = "relu", input_shape = c(260, 3)) %>%
    layer_max_pooling_1d(pool_size = 2) %>%
    layer_conv_1d(filters = 40, kernel_size = 10, activation = "relu") %>%
    layer_max_pooling_1d(pool_size = 2) %>%
    layer_flatten() %>%
    layer_dense(units = 100, activation = "sigmoid") %>%
    layer_dense(units = 15, activation = "softmax")

model
```

```{r}
model %>%
    compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = c("accuracy"))
```

```{r}
history <- model %>%
    fit(X_train, y_train, epochs = 10, batch_size = 100, validation_data = list(X_val, y_val), verbose = 1)
```

At this point we could try to improve that cross-validation accuracy score, e.g. change network structure.


## Reporting


```{r}
y_pred <- model %>%
    # For some reason, even though this is R, we predict classes between 0 and 14...
    predict_classes(X_test)


classification_report <- function(target, predictions) {
    
    # Confusion matrix
    confusion <- table(Actual = target, Predicted = predictions)

    # Precision
    precision <- diag(confusion) / colSums(confusion)

    # Recall
    recall <- diag(confusion) / rowSums(confusion)

    # F1 score
    f1 <- 2*precision*recall / (precision+recall)
    
    metrics <- data.frame("Precision" = precision,
                          "Recall" = recall,
                          "F1_score" = f1) %>%
        round(4) %>%
        rbind(c("---", "---", "---")) %>%
        rbind(round(c(mean(precision), mean(recall), mean(f1)), 4))
    
    rownames(metrics) <- c(paste("Class", seq_along(f1)), "---", "Mean")
    
    list("performance_metrics" = metrics,
         "confusion_matrix" = confusion)
}


classification_report(max.col(y_test) - 1, y_pred)
```



## Visualising features

We can try to visualise the "features" of the time series which the convolutional layers of the net have learned to identify.

```{r}
plot_filter <- function(model, layer, k) {
    
    # Get weights from target filters in specified layer
    weights <- get_weights(model$layers[[layer]])[[1]][, , k]
    plot_series(weights)
}


model %>%
    plot_filter(1, 1)

model %>%
    plot_filter(1, 12)

# Up one more level of abstraction...
model %>%
    plot_filter(3, 1)
```


We can also see if there are any patterns in the autocorrelation plots which might suggest strong periodicity.


```{r}
plot_filter_corr <- function(model, layer, k) {
    
    weights <- get_weights(model$layers[[layer]])[[1]][, , k]

    corrs <- apply(weights, 2, function(x) {
        acf(x, lag.max = nrow(weights), plot = FALSE)[["acf"]]
    })
    
    plot_series(corrs)
}

    
model %>%
    plot_filter_corr(1, 1)
```



```{r}
model %>%
    plot_filter(1, 1:6)

model %>%
    plot_filter_corr(1, 1:6)
```

